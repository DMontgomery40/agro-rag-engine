"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[695],{325:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>a,frontMatter:()=>d,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"configuration/models","title":"Model Configuration","description":"Complete guide to choosing embedding and inference models for AGRO based on your requirements, budget, and hardware.","source":"@site/docs/configuration/models.md","sourceDirName":"configuration","slug":"/configuration/models","permalink":"/agro-rag-engine/configuration/models","draft":false,"unlisted":false,"editUrl":"https://github.com/DMontgomery40/agro-rag-engine/tree/main/website/docs/configuration/models.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docs","previous":{"title":"MCP Tools","permalink":"/agro-rag-engine/api/mcp-tools"},"next":{"title":"Performance & Cost Analysis","permalink":"/agro-rag-engine/configuration/performance"}}');var r=i(4848),l=i(8453);const d={sidebar_position:1},t="Model Configuration",c={},o=[{value:"Quick Decision Matrix",id:"quick-decision-matrix",level:2},{value:"Cloud Models (API-Based)",id:"cloud-models-api-based",level:2},{value:"Current API Pricing",id:"current-api-pricing",level:3},{value:"OpenAI (Updated October 2025)",id:"openai-updated-october-2025",level:4},{value:"Google Gemini (Updated October 2025)",id:"google-gemini-updated-october-2025",level:4},{value:"Anthropic Claude (Updated October 2025)",id:"anthropic-claude-updated-october-2025",level:4},{value:"Cloud Model Recommendations",id:"cloud-model-recommendations",level:3},{value:"Best Overall Value: Gemini 2.5 Flash",id:"best-overall-value-gemini-25-flash",level:4},{value:"Best Code Quality: GPT-4o mini",id:"best-code-quality-gpt-4o-mini",level:4},{value:"Best for Large Context: Gemini 2.5 Flash",id:"best-for-large-context-gemini-25-flash",level:4},{value:"Self-Hosted Models",id:"self-hosted-models",level:2},{value:"Embedding Models",id:"embedding-models",level:3},{value:"nomic-embed-text (Recommended for Mac)",id:"nomic-embed-text-recommended-for-mac",level:4},{value:"BGE-M3 (Multilingual, High Quality)",id:"bge-m3-multilingual-high-quality",level:4},{value:"NV-Embed-v2 (NVIDIA GPUs)",id:"nv-embed-v2-nvidia-gpus",level:4},{value:"Inference Models",id:"inference-models",level:3},{value:"Qwen3-Coder (Recommended)",id:"qwen3-coder-recommended",level:4},{value:"MLX vs Ollama (Apple Silicon)",id:"mlx-vs-ollama-apple-silicon",level:4},{value:"DeepSeek-Coder V3",id:"deepseek-coder-v3",level:4},{value:"Hardware-Specific Recommendations",id:"hardware-specific-recommendations",level:2},{value:"Apple Silicon Macs",id:"apple-silicon-macs",level:3},{value:"M1/M2 (8-16GB RAM)",id:"m1m2-8-16gb-ram",level:4},{value:"M3/M4 (16-32GB RAM)",id:"m3m4-16-32gb-ram",level:4},{value:"M4 Pro/Max (32GB+ RAM)",id:"m4-promax-32gb-ram",level:4},{value:"NVIDIA GPUs",id:"nvidia-gpus",level:3},{value:"RTX 3080/4080 (8-16GB VRAM)",id:"rtx-30804080-8-16gb-vram",level:4},{value:"RTX 3090/4090 (24GB VRAM)",id:"rtx-30904090-24gb-vram",level:4},{value:"A100/H100 (40GB+ VRAM)",id:"a100h100-40gb-vram",level:4},{value:"CPU-Only (Budget/Privacy)",id:"cpu-only-budgetprivacy",level:3},{value:"16-32GB RAM",id:"16-32gb-ram",level:4},{value:"Benchmark References",id:"benchmark-references",level:2},{value:"Embedding Benchmarks",id:"embedding-benchmarks",level:3},{value:"Code Generation Benchmarks",id:"code-generation-benchmarks",level:3},{value:"Migration Guides",id:"migration-guides",level:2},{value:"Switch to Fully Local (Mac Example)",id:"switch-to-fully-local-mac-example",level:3},{value:"Switch to Budget Cloud (Gemini)",id:"switch-to-budget-cloud-gemini",level:3},{value:"Final Recommendations by Scenario",id:"final-recommendations-by-scenario",level:2},{value:"Startup/Prototype (Minimize Cost)",id:"startupprototype-minimize-cost",level:3},{value:"Production (Balance Cost/Quality)",id:"production-balance-costquality",level:3},{value:"Enterprise (Best Quality, On-Prem)",id:"enterprise-best-quality-on-prem",level:3},{value:"Privacy-Critical (Airgapped)",id:"privacy-critical-airgapped",level:3},{value:"Additional Resources",id:"additional-resources",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"model-configuration",children:"Model Configuration"})}),"\n",(0,r.jsx)(n.p,{children:"Complete guide to choosing embedding and inference models for AGRO based on your requirements, budget, and hardware."}),"\n",(0,r.jsx)(n.admonition,{title:"AI Model Pricing Changes Daily",type:"warning",children:(0,r.jsxs)(n.p,{children:["These recommendations are current as of ",(0,r.jsx)(n.strong,{children:"October 8, 2025"}),", but will become outdated quickly. Always verify pricing and performance at official sources before making decisions."]})}),"\n",(0,r.jsx)(n.h2,{id:"quick-decision-matrix",children:"Quick Decision Matrix"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Use Case"}),(0,r.jsx)(n.th,{children:"Embedding"}),(0,r.jsx)(n.th,{children:"Inference"}),(0,r.jsx)(n.th,{children:"Monthly Cost*"}),(0,r.jsx)(n.th,{children:"Hardware"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Production (Cloud)"})}),(0,r.jsx)(n.td,{children:"OpenAI text-embedding-3-large"}),(0,r.jsx)(n.td,{children:"GPT-4o mini"}),(0,r.jsx)(n.td,{children:"$5"}),(0,r.jsx)(n.td,{children:"None"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Budget Cloud"})}),(0,r.jsx)(n.td,{children:"Google Gemini (free tier)"}),(0,r.jsx)(n.td,{children:"Gemini 2.5 Flash"}),(0,r.jsx)(n.td,{children:"$1"}),(0,r.jsx)(n.td,{children:"None"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Premium Cloud"})}),(0,r.jsx)(n.td,{children:"OpenAI text-embedding-3-large"}),(0,r.jsx)(n.td,{children:"Claude Sonnet 4.5"}),(0,r.jsx)(n.td,{children:"$10"}),(0,r.jsx)(n.td,{children:"None"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Self-Hosted (Mac M-series)"})}),(0,r.jsx)(n.td,{children:"nomic-embed-text"}),(0,r.jsx)(n.td,{children:"Qwen3-Coder 30B"}),(0,r.jsx)(n.td,{children:"$0"}),(0,r.jsx)(n.td,{children:"32GB+ RAM"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Self-Hosted (NVIDIA)"})}),(0,r.jsx)(n.td,{children:"NV-Embed-v2"}),(0,r.jsx)(n.td,{children:"DeepSeek-Coder V2"}),(0,r.jsx)(n.td,{children:"$0"}),(0,r.jsx)(n.td,{children:"24GB+ VRAM"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Privacy First"})}),(0,r.jsx)(n.td,{children:"BGE-M3 (local)"}),(0,r.jsx)(n.td,{children:"Qwen3-Coder (local)"}),(0,r.jsx)(n.td,{children:"$0"}),(0,r.jsx)(n.td,{children:"16GB+ RAM"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"*Estimated for high usage (1M tokens/month). AGRO caches by default and only re-embeds changed code."}),"\n",(0,r.jsx)(n.h2,{id:"cloud-models-api-based",children:"Cloud Models (API-Based)"}),"\n",(0,r.jsx)(n.admonition,{title:"Important Note",type:"tip",children:(0,r.jsx)(n.p,{children:"You do NOT need the same level of inference model that you would use without a good RAG. AGRO does a lot of the intelligence work upfront through semantic search and reranking."})}),"\n",(0,r.jsx)(n.h3,{id:"current-api-pricing",children:"Current API Pricing"}),"\n",(0,r.jsxs)(n.admonition,{title:"Prices Change Frequently",type:"warning",children:[(0,r.jsx)(n.p,{children:"Verify current pricing at:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenAI"}),": ",(0,r.jsx)(n.a,{href:"https://platform.openai.com/pricing",children:"https://platform.openai.com/pricing"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Google"}),": ",(0,r.jsx)(n.a,{href:"https://ai.google.dev/pricing",children:"https://ai.google.dev/pricing"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Anthropic"}),": ",(0,r.jsx)(n.a,{href:"https://docs.anthropic.com/pricing",children:"https://docs.anthropic.com/pricing"})]}),"\n"]})]}),"\n",(0,r.jsx)(n.h4,{id:"openai-updated-october-2025",children:"OpenAI (Updated October 2025)"}),"\n",(0,r.jsx)(n.admonition,{title:"API Change",type:"info",children:(0,r.jsx)(n.p,{children:"OpenAI has deprecated the Chat Completions API in favor of the Responses API. If an LLM recommends Chat Completions, note this is outdated."})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Input (per 1M tokens)"}),(0,r.jsx)(n.th,{children:"Output (per 1M tokens)"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"GPT-4o mini"})}),(0,r.jsx)(n.td,{children:"$0.15"}),(0,r.jsx)(n.td,{children:"$0.60"}),(0,r.jsx)(n.td,{children:"Best value for code generation"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"GPT-4o"})}),(0,r.jsx)(n.td,{children:"$2.50"}),(0,r.jsx)(n.td,{children:"$10.00"}),(0,r.jsx)(n.td,{children:"Production, complex reasoning"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"o3-mini"})}),(0,r.jsx)(n.td,{children:"$0.40"}),(0,r.jsx)(n.td,{children:"$1.60"}),(0,r.jsx)(n.td,{children:"Fast reasoning (new)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"text-embedding-3-large"})}),(0,r.jsx)(n.td,{children:"$0.13"}),(0,r.jsx)(n.td,{children:"N/A"}),(0,r.jsx)(n.td,{children:"High-quality embeddings (3072d)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"text-embedding-3-small"})}),(0,r.jsx)(n.td,{children:"$0.02"}),(0,r.jsx)(n.td,{children:"N/A"}),(0,r.jsx)(n.td,{children:"Budget embeddings (1536d)"})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"google-gemini-updated-october-2025",children:"Google Gemini (Updated October 2025)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Input (per 1M tokens)"}),(0,r.jsx)(n.th,{children:"Output (per 1M tokens)"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Gemini 2.5 Flash"})}),(0,r.jsx)(n.td,{children:"$0.075"}),(0,r.jsx)(n.td,{children:"$0.30"}),(0,r.jsx)(n.td,{children:"Best value, 1M context"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Gemini 2.5 Pro"})}),(0,r.jsx)(n.td,{children:"$1.25"}),(0,r.jsx)(n.td,{children:"$5.00"}),(0,r.jsx)(n.td,{children:"Advanced reasoning"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Gemini Embeddings"})}),(0,r.jsx)(n.td,{children:"$0.15"}),(0,r.jsx)(n.td,{children:"N/A"}),(0,r.jsx)(n.td,{children:"Free tier available (768d)"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Free Tier"}),": Gemini offers generous free limits for embeddings. Check current quotas at ",(0,r.jsx)(n.a,{href:"https://ai.google.dev/pricing",children:"https://ai.google.dev/pricing"})]}),"\n",(0,r.jsx)(n.h4,{id:"anthropic-claude-updated-october-2025",children:"Anthropic Claude (Updated October 2025)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Input (per 1M tokens)"}),(0,r.jsx)(n.th,{children:"Output (per 1M tokens)"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Claude Haiku 3.5"})}),(0,r.jsx)(n.td,{children:"$0.80"}),(0,r.jsx)(n.td,{children:"$4.00"}),(0,r.jsx)(n.td,{children:"Fast, economical"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Claude Sonnet 4.5"})}),(0,r.jsx)(n.td,{children:"$3.00"}),(0,r.jsx)(n.td,{children:"$15.00"}),(0,r.jsx)(n.td,{children:"Balanced performance"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Claude Opus 4.1"})}),(0,r.jsx)(n.td,{children:"$15.00"}),(0,r.jsx)(n.td,{children:"$75.00"}),(0,r.jsx)(n.td,{children:"Highest quality"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"cloud-model-recommendations",children:"Cloud Model Recommendations"}),"\n",(0,r.jsx)(n.h4,{id:"best-overall-value-gemini-25-flash",children:"Best Overall Value: Gemini 2.5 Flash"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": $0.075/$0.30 pricing with 1M token context"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"When"}),": Production RAG with moderate budgets"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Caveat"}),": Slightly lower code quality than GPT-4o mini"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"best-code-quality-gpt-4o-mini",children:"Best Code Quality: GPT-4o mini"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": 87.2% HumanEval score, $0.15/$0.60 pricing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"When"}),": Code generation is critical"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Caveat"}),": More expensive than Gemini"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"best-for-large-context-gemini-25-flash",children:"Best for Large Context: Gemini 2.5 Flash"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": 1M token context (8x more than GPT-4o mini)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"When"}),": Processing large codebases or documents"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Caveat"}),": Requires careful prompt engineering"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"self-hosted-models",children:"Self-Hosted Models"}),"\n",(0,r.jsx)(n.h3,{id:"embedding-models",children:"Embedding Models"}),"\n",(0,r.jsx)(n.h4,{id:"nomic-embed-text-recommended-for-mac",children:"nomic-embed-text (Recommended for Mac)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dimensions"}),": 768"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size"}),": 274MB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),": 71% MTEB (surpasses OpenAI ada-002)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware"}),": 8GB+ RAM (Mac M-series optimized)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speed"}),": Very fast on Apple Silicon"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Setup"}),": ",(0,r.jsx)(n.code,{children:"ollama pull nomic-embed-text"})]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"bge-m3-multilingual-high-quality",children:"BGE-M3 (Multilingual, High Quality)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dimensions"}),": 1024"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size"}),": ~1.3GB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),": 71% MTEB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware"}),": 16GB+ RAM"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Languages"}),": 100+"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Setup"}),": Via sentence-transformers"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"nv-embed-v2-nvidia-gpus",children:"NV-Embed-v2 (NVIDIA GPUs)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dimensions"}),": 1024"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size"}),": ~7GB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),": 72.31% MTEB (former #1)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware"}),": NVIDIA GPU with 16GB+ VRAM"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speed"}),": Optimized for CUDA"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Setup"}),": Via transformers with ",(0,r.jsx)(n.code,{children:"trust_remote_code"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"inference-models",children:"Inference Models"}),"\n",(0,r.jsx)(n.h4,{id:"qwen3-coder-recommended",children:"Qwen3-Coder (Recommended)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Variants"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"7B"})," (~4GB): 8GB+ RAM"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"14B"})," (~8GB): 16GB+ RAM"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"30B"})," (~18GB): 32GB+ RAM (Mac M4 Max recommended)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),": Excellent on code (leads open-source)\n",(0,r.jsx)(n.strong,{children:"Context"}),": 256K tokens\n",(0,r.jsx)(n.strong,{children:"Speed"}),": 100+ tokens/sec on M4 Max"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Setup (Ollama)"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ollama pull qwen3-coder:30b\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Setup (MLX - Apple Silicon)"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install mlx mlx-lm\npython -c \"from mlx_lm import load; load('mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit')\"\n"})}),"\n",(0,r.jsx)(n.h4,{id:"mlx-vs-ollama-apple-silicon",children:"MLX vs Ollama (Apple Silicon)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"MLX"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Direct Metal GPU integration"}),"\n",(0,r.jsx)(n.li,{children:"Optimized for Apple Silicon unified memory"}),"\n",(0,r.jsx)(n.li,{children:"Uses GPU, NOT Neural Engine (ANE is for smaller CoreML models)"}),"\n",(0,r.jsx)(n.li,{children:"Better memory efficiency"}),"\n",(0,r.jsxs)(n.li,{children:["Setup: ",(0,r.jsx)(n.code,{children:"pip install mlx mlx-lm"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Ollama"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Also uses Metal GPU on Apple Silicon"}),"\n",(0,r.jsx)(n.li,{children:"Similar thermal profile to MLX"}),"\n",(0,r.jsx)(n.li,{children:"Easier multi-platform support"}),"\n",(0,r.jsx)(n.li,{children:"More straightforward model management"}),"\n",(0,r.jsxs)(n.li,{children:["Setup: ",(0,r.jsx)(n.code,{children:"brew install ollama"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Verdict"}),": Both are good options. MLX may have a slight memory efficiency edge, Ollama is more portable."]}),"\n",(0,r.jsx)(n.h4,{id:"deepseek-coder-v3",children:"DeepSeek-Coder V3"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Variants"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"16B"})," (~9GB): 16GB+ RAM or 12GB+ VRAM"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"236B"})," (multi-GPU): 80GB+ VRAM"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),": 85.6% HumanEval (highest open-source)\n",(0,r.jsx)(n.strong,{children:"Context"}),": 16K tokens"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Setup"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ollama pull deepseek-coder-v2:16b\n"})}),"\n",(0,r.jsx)(n.h2,{id:"hardware-specific-recommendations",children:"Hardware-Specific Recommendations"}),"\n",(0,r.jsx)(n.h3,{id:"apple-silicon-macs",children:"Apple Silicon Macs"}),"\n",(0,r.jsx)(n.h4,{id:"m1m2-8-16gb-ram",children:"M1/M2 (8-16GB RAM)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": nomic-embed-text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": Qwen3-Coder 7B"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Optimized for unified memory, fits in RAM"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"m3m4-16-32gb-ram",children:"M3/M4 (16-32GB RAM)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": nomic-embed-text or BGE-M3"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": Qwen3-Coder 14B"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": More headroom for larger models"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"m4-promax-32gb-ram",children:"M4 Pro/Max (32GB+ RAM)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": BGE-M3"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": Qwen3-Coder 30B"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Can handle state-of-the-art local models"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"nvidia-gpus",children:"NVIDIA GPUs"}),"\n",(0,r.jsx)(n.h4,{id:"rtx-30804080-8-16gb-vram",children:"RTX 3080/4080 (8-16GB VRAM)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": BGE-large"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": Qwen3-Coder 14B or DeepSeek-Coder 16B"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Good balance for mid-range GPUs"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"rtx-30904090-24gb-vram",children:"RTX 3090/4090 (24GB VRAM)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": NV-Embed-v2"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": Qwen3-Coder 30B or DeepSeek-Coder 16B"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Full utilization of high-end consumer GPUs"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"a100h100-40gb-vram",children:"A100/H100 (40GB+ VRAM)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": NV-Embed-v2"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": DeepSeek-Coder V2 236B"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Datacenter-grade, best local performance"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"cpu-only-budgetprivacy",children:"CPU-Only (Budget/Privacy)"}),"\n",(0,r.jsx)(n.h4,{id:"16-32gb-ram",children:"16-32GB RAM"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": BGE-small-en-v1.5"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": Qwen3-Coder 7B (quantized)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": CPU inference is slow, use smallest viable models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Note"}),": Expect 5-10x slower than GPU"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"benchmark-references",children:"Benchmark References"}),"\n",(0,r.jsx)(n.admonition,{title:"Benchmarks Change Daily",type:"warning",children:(0,r.jsx)(n.p,{children:"These links show current leaderboards. Rankings shift as new models release."})}),"\n",(0,r.jsx)(n.h3,{id:"embedding-benchmarks",children:"Embedding Benchmarks"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MTEB Leaderboard"})," (Primary): ",(0,r.jsx)(n.a,{href:"https://huggingface.co/spaces/mteb/leaderboard",children:"https://huggingface.co/spaces/mteb/leaderboard"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NVIDIA Blog"}),": ",(0,r.jsx)(n.a,{href:"https://developer.nvidia.com/blog/nvidia-text-embedding-model-tops-mteb-leaderboard/",children:"https://developer.nvidia.com/blog/nvidia-text-embedding-model-tops-mteb-leaderboard/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Nomic Analysis"}),": ",(0,r.jsx)(n.a,{href:"https://www.nomic.ai/blog/posts/evaluating-embedding-models",children:"https://www.nomic.ai/blog/posts/evaluating-embedding-models"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"code-generation-benchmarks",children:"Code Generation Benchmarks"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"HumanEval Stats"}),": ",(0,r.jsx)(n.a,{href:"https://llm-stats.com/benchmarks/humaneval",children:"https://llm-stats.com/benchmarks/humaneval"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiveBench"}),": ",(0,r.jsx)(n.a,{href:"https://livebench.ai/",children:"https://livebench.ai/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vellum LLM Leaderboard"}),": ",(0,r.jsx)(n.a,{href:"https://www.vellum.ai/llm-leaderboard",children:"https://www.vellum.ai/llm-leaderboard"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Aider Leaderboards"}),": ",(0,r.jsx)(n.a,{href:"https://aider.chat/docs/leaderboards/",children:"https://aider.chat/docs/leaderboards/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"migration-guides",children:"Migration Guides"}),"\n",(0,r.jsx)(n.h3,{id:"switch-to-fully-local-mac-example",children:"Switch to Fully Local (Mac Example)"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Install Ollama"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"brew install ollama\nollama serve  # Keep running\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Pull Models"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Embedding\nollama pull nomic-embed-text\n\n# Inference (choose based on RAM)\nollama pull qwen3-coder:7b   # 8-16GB\nollama pull qwen3-coder:14b  # 16-32GB\nollama pull qwen3-coder:30b  # 32GB+\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Update .env"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Generation\nOLLAMA_URL=http://127.0.0.1:11434/api\nGEN_MODEL=qwen3-coder:30b\n\n# Embeddings (optional, uses Ollama)\nEMBEDDING_TYPE=local  # Falls back to Ollama/local models\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. Re-index"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"REPO=agro python index_repo.py\n"})}),"\n",(0,r.jsx)(n.h3,{id:"switch-to-budget-cloud-gemini",children:"Switch to Budget Cloud (Gemini)"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Get API Key"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Visit: ",(0,r.jsx)(n.a,{href:"https://makersuite.google.com/app/apikey",children:"https://makersuite.google.com/app/apikey"})]}),"\n",(0,r.jsx)(n.li,{children:"Create key"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Update .env"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"GOOGLE_API_KEY=your_key_here\nEMBEDDING_TYPE=gemini  # If implementing Gemini embeddings\nGEN_MODEL=gemini-2.5-flash\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"3. Update Code"})," (if not already supported)"]}),"\n",(0,r.jsx)(n.p,{children:"See implementation examples in the code. Gemini support may need custom integration."}),"\n",(0,r.jsx)(n.h2,{id:"final-recommendations-by-scenario",children:"Final Recommendations by Scenario"}),"\n",(0,r.jsx)(n.h3,{id:"startupprototype-minimize-cost",children:"Startup/Prototype (Minimize Cost)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": Google Gemini (free tier)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": Gemini 2.5 Flash"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": $3-10/month for moderate usage"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"When to Switch"}),": When free tier limits hit or need better quality"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"production-balance-costquality",children:"Production (Balance Cost/Quality)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": OpenAI text-embedding-3-large"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": GPT-4o mini or Gemini 2.5 Flash"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Proven reliability, good performance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": $50-200/month depending on scale"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"enterprise-best-quality-on-prem",children:"Enterprise (Best Quality, On-Prem)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": NV-Embed-v2 or BGE-M3"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": Qwen3-Coder 30B or DeepSeek-Coder V2"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Complete data control, zero API costs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": Hardware only ($1.4K-$3K Mac or GPU server)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"privacy-critical-airgapped",children:"Privacy-Critical (Airgapped)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": BGE-M3 (multilingual)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": Qwen3-Coder 14B"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Why"}),": Fully offline, no external APIs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": Hardware only, works on M-series Mac or mid-range PC"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenAI Pricing"}),": ",(0,r.jsx)(n.a,{href:"https://platform.openai.com/pricing",children:"https://platform.openai.com/pricing"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Google AI Pricing"}),": ",(0,r.jsx)(n.a,{href:"https://ai.google.dev/pricing",children:"https://ai.google.dev/pricing"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Anthropic Pricing"}),": ",(0,r.jsx)(n.a,{href:"https://docs.anthropic.com/pricing",children:"https://docs.anthropic.com/pricing"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MTEB Leaderboard"}),": ",(0,r.jsx)(n.a,{href:"https://huggingface.co/spaces/mteb/leaderboard",children:"https://huggingface.co/spaces/mteb/leaderboard"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"HumanEval Benchmarks"}),": ",(0,r.jsx)(n.a,{href:"https://llm-stats.com/benchmarks/humaneval",children:"https://llm-stats.com/benchmarks/humaneval"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ollama Models"}),": ",(0,r.jsx)(n.a,{href:"https://ollama.com/library",children:"https://ollama.com/library"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Continue.dev Embedding Guide"}),": ",(0,r.jsx)(n.a,{href:"https://docs.continue.dev/customize/model-roles/embeddings",children:"https://docs.continue.dev/customize/model-roles/embeddings"})]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Last Updated"}),": October 8, 2025\n",(0,r.jsx)(n.strong,{children:"Next Review"}),": Check pricing/benchmarks before implementing - they change daily!"]})]})}function a(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>d,x:()=>t});var s=i(6540);const r={},l=s.createContext(r);function d(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);