{"query":"Launch indexer with correct interpreter and REPO_ROOT","positive_doc_id":"server/services/indexing.py:24-40","positive_text":"repo = os.getenv(\"REPO\", \"agro\")\n_INDEX_STATUS.append(f\"Indexing repository: {repo}\")\n# Ensure the indexer resolves repo paths correctly and uses the same interpreter\nenv = {**os.environ, \"REPO\": repo, \"REPO_ROOT\": str(repo_root())}\nif payload.get(\"enrich\"):\n    env[\"ENRICH_CODE_CHUNKS\"] = \"true\"\n    _INDEX_STATUS.append(\"Enriching chunks with summaries and keywords...\")\nif payload.get(\"skip_dense\"):\n    env[\"SKIP_DENSE\"] = \"1\"\n    _INDEX_STATUS.append(\"Skipping dense embeddings (BM25 only)...\")\nresult = subprocess.run(\n    [sys.executable, \"-m\", \"indexer.index_repo\"],\n    capture_output=True,\n    text=True,\n    cwd=repo_root(),\n    env=env\n)","negative_doc_ids":["server/app.py:1950-1969"],"negative_texts":["# Prepare environment\nenv = {**os.environ, \"REPO\": repo}\n# ...\nresult = subprocess.run(\n    [sys.executable, \"-m\", \"indexer.index_repo\"],\n    capture_output=True,\n    text=True,\n    cwd=repo_root(),\n    env=env\n)"]}
{"query":"Router endpoint for /api/index/start","positive_doc_id":"server/routers/indexing.py:13-15","positive_text":"@router.post(\"/api/index/start\")\ndef index_start(payload: Dict[str, Any] = None) -> Dict[str, Any]:\n    return svc.start(payload)","negative_doc_ids":["server/services/indexing.py:15-23"],"negative_texts":["def start(payload: Dict[str, Any] | None = None) -> Dict[str, Any]:\n    global _INDEX_STATUS, _INDEX_METADATA\n    payload = payload or {}\n    _INDEX_STATUS = [\"Indexing started...\"]\n    _INDEX_METADATA = {}\n\n    def run_index():\n        global _INDEX_STATUS, _INDEX_METADATA\n        try:"]}
{"query":"Compute BM25 vocab and save directory (chunks corpus)","positive_doc_id":"indexer/index_repo.py:323-335","positive_text":"stemmer = Stemmer('english')\ntokenizer = Tokenizer(stemmer=stemmer, stopwords='en')\ncorpus_tokens = tokenizer.tokenize(corpus)\nretriever = bm25s.BM25(method='lucene', k1=1.2, b=0.65)\nretriever.index(corpus_tokens)\nos.makedirs(os.path.join(OUTDIR, 'bm25_index'), exist_ok=True)\ntry:\n    retriever.vocab_dict = {str(k): v for k, v in retriever.vocab_dict.items()}\nexcept Exception:\n    pass\nretriever.save(os.path.join(OUTDIR, 'bm25_index'), corpus=corpus)\ntokenizer.save_vocab(save_dir=os.path.join(OUTDIR, 'bm25_index'))\ntokenizer.save_stopwords(save_dir=os.path.join(OUTDIR, 'bm25_index'))","negative_doc_ids":["server/cards_builder.py:320-333"],"negative_texts":["stemmer = Stemmer(\"english\")\ntok = Tokenizer(stemmer=stemmer, stopwords=\"en\")\ndocs = [ln.strip() for ln in paths[\"cards_txt\"].read_text(encoding=\"utf-8\").splitlines() if ln.strip()]\ntokens = tok.tokenize(docs)\nretriever = bm25s.BM25(method=\"lucene\", k1=1.2, b=0.65)\nretriever.index(tokens)\ntry:\n    retriever.vocab_dict = {str(k): v for k, v in retriever.vocab_dict.items()}\nexcept Exception:\n    pass\npaths[\"bm25_dir\"].mkdir(parents=True, exist_ok=True)\nretriever.save(str(paths[\"bm25_dir\"]))\ntok.save_vocab(save_dir=str(paths[\"bm25_dir\"]))\ntok.save_stopwords(save_dir=str(paths[\"bm25_dir\"]))"]}
{"query":"Persist BM25 corpus + chunk id map (indexer)","positive_doc_id":"indexer/index_repo.py:336-346","positive_text":"with open(os.path.join(OUTDIR, 'bm25_index', 'corpus.txt'), 'w', encoding='utf-8') as f:\n    for doc in corpus:\n        f.write(doc.replace('\\n','\\\\n')+'\\n')\nchunk_ids = [str(c['id']) for c in chunks]\nwith open(os.path.join(OUTDIR, 'bm25_index', 'chunk_ids.txt'), 'w', encoding='utf-8') as f:\n    for cid in chunk_ids:\n        f.write(cid+'\\n')\nimport json as _json\n_json.dump({str(i): cid for i, cid in enumerate(chunk_ids)}, open(os.path.join(OUTDIR,'bm25_index','bm25_map.json'),'w'))\nwith open(os.path.join(OUTDIR,'chunks.jsonl'),'w',encoding='utf-8') as f:\n    for c in chunks:\n        f.write(json.dumps(c, ensure_ascii=False)+'\\n')","negative_doc_ids":["indexer/index_repo.py:442-443"],"negative_texts":["import json as _json\n_json.dump({str(i): pid for i, pid in enumerate(point_ids)}, open(os.path.join(OUTDIR,'bm25_index','bm25_point_ids.json'),'w'))"]}
{"query":"Skip dense path when SKIP_DENSE=1","positive_doc_id":"indexer/index_repo.py:364-366","positive_text":"if (os.getenv('SKIP_DENSE','0') or '0').strip() == '1':\n    print('Skipping dense embeddings and Qdrant upsert (SKIP_DENSE=1).')\n    return","negative_doc_ids":["server/services/indexing.py:31-35"],"negative_texts":["if payload.get(\"skip_dense\"):\n    env[\"SKIP_DENSE\"] = \"1\"\n    _INDEX_STATUS.append(\"Skipping dense embeddings (BM25 only)...\")\nresult = subprocess.run(\n    [sys.executable, \"-m\", \"indexer.index_repo\"],"]}
{"query":"Qdrant: create collection with correct vector size","positive_doc_id":"indexer/index_repo.py:413-418","positive_text":"q = QdrantClient(url=QDRANT_URL)\nqdrant_recreate_fallback.recreate_collection(\n    q,\n    collection_name=COLLECTION,\n    vectors_config={'dense': models.VectorParams(size=len(embs[0]), distance=models.Distance.COSINE)}\n)","negative_doc_ids":["server/index_stats.py:62-75"],"negative_texts":["# Get embedding configuration\nembedding_type = os.getenv(\"EMBEDDING_TYPE\", \"openai\").lower()\nembedding_dim = int(os.getenv(\"EMBEDDING_DIM\", \"3072\" if embedding_type == \"openai\" else \"512\"))\n# ... reporting-only embedding_config"]}
{"query":"Embedding provider routing and fallbacks","positive_doc_id":"indexer/index_repo.py:377-410","positive_text":"et = (os.getenv('EMBEDDING_TYPE','openai') or 'openai').lower()\nif et == 'voyage':\n    try:\n        voyage_model = os.getenv('VOYAGE_MODEL', 'voyage-code-3')\n        embs = embed_texts_voyage(texts, model=voyage_model, batch=64, output_dimension=int(os.getenv('VOYAGE_EMBED_DIM','512')))\n    except Exception as e:\n        print(f\"Voyage embedding failed ({e}); falling back to local embeddings.\")\n        embs = []\n    if not embs:\n        embs = embed_texts_local(texts)\nelif et == 'mxbai':\n    try:\n        dim = int(os.getenv('EMBEDDING_DIM', '512'))\n        embs = embed_texts_mxbai(texts, dim=dim)\n    except Exception as e:\n        print(f\"MXBAI embedding failed ({e}); falling back to local embeddings.\")\n        embs = embed_texts_local(texts)\nelif et == 'local':\n    embs = embed_texts_local(texts)\nelse:\n    if client is not None:\n        try:\n            cache = EmbeddingCache(OUTDIR)\n            hashes = [c['hash'] for c in chunks]\n            embedding_model = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-large')\n            embs = cache.embed_texts(client, texts, hashes, model=embedding_model, batch=64)\n            pruned = cache.prune(set(hashes))\n            if pruned > 0:\n                print(f'Pruned {pruned} orphaned embeddings from cache.')\n            cache.save()\n        except Exception as e:\n            print(f'Embedding via OpenAI failed ({e}); falling back to local embeddings.')\n    if not embs:\n        embs = embed_texts_local(texts)","negative_doc_ids":["server/index_stats.py:70-75"],"negative_texts":["\"embedding_config\": {\n    \"provider\": embedding_type,\n    \"model\": \"text-embedding-3-large\" if embedding_type == \"openai\" else f\"local-{embedding_type}\",\n    \"dimensions\": embedding_dim,\n    \"precision\": \"float32\",\n},"]}
{"query":"Clip inputs for OpenAI embeddings (~8k tokens)","positive_doc_id":"indexer/index_repo.py:189-200","positive_text":"def _clip_for_openai(text: str, enc, max_tokens: int = 8000) -> str:\n    toks = enc.encode(text)\n    if len(toks) <= max_tokens:\n        return text\n    return enc.decode(toks[:max_tokens])\n\ndef embed_texts(client: OpenAI, texts: List[str], model: str = 'text-embedding-3-large', batch: int = 64) -> List[List[float]]:\n    embs = []\n    enc = tiktoken.get_encoding('cl100k_base')\n    for i in range(0, len(texts), batch):\n        sub = [_clip_for_openai(t, enc) for t in texts[i:i+batch]]\n        r = client.embeddings.create(model=model, input=sub)","negative_doc_ids":["retrieval/ast_chunker.py:22-28"],"negative_texts":["OVERLAP_LINES = 20\n\nFUNC_NODES = {\n    \"python\": {\"function_definition\", \"class_definition\"},\n    \"javascript\": {\"function_declaration\", \"class_declaration\", \"method_definition\", \"arrow_function\"},"]}
{"query":"Repo-aware layer tagging (gui/server/retrieval/...)","positive_doc_id":"indexer/index_repo.py:148-166","positive_text":"def detect_layer(fp: str) -> str:\n    f = (fp or '').lower()\n    if '/gui/' in f or '/public/' in f:\n        return 'gui'\n    if '/server/' in f:\n        return 'server'\n    if '/retrieval/' in f:\n        return 'retrieval'\n    if '/indexer/' in f:\n        return 'indexer'\n    if '/eval/' in f or '/tests/' in f:\n        return 'eval'\n    if '/scripts/' in f:\n        return 'scripts'\n    if '/common/' in f:\n        return 'common'\n    if '/infra/' in f:\n        return 'infra'\n    return 'server'","negative_doc_ids":["indexer/index_repo.py:172-186"],"negative_texts":["def detect_origin(fp: str) -> str:\n    low = (fp or '').lower()\n    for m in VENDOR_MARKERS:\n        if m in low:\n            return 'vendor'\n    try:\n        with open(fp, 'r', encoding='utf-8', errors='ignore') as f:\n            head = ''.join([next(f) for _ in range(12)])\n        if any(k in head.lower() for k in (\n            'apache license','mit license','bsd license','mozilla public license'\n        )):\n            return 'vendor'\n    except Exception:\n        pass\n    return 'first_party'"]}
{"query":"Respect repo-specific exclude patterns during indexing","positive_doc_id":"indexer/index_repo.py:242-255","positive_text":"# Load repo-specific exclude patterns from repos.json\nrepo_exclude_patterns = exclude_paths(REPO)\nif repo_exclude_patterns:\n    print(f'Loaded {len(repo_exclude_patterns)} exclude patterns for repo \"{REPO}\": {repo_exclude_patterns}')\n\nfiles = collect_files(BASES)\nprint(f'Discovered {len(files)} source files.')\nall_chunks: List[Dict] = []\nfor fp in files:\n    if not should_index_file(fp, repo_exclude_patterns):\n        continue\n    lang = lang_from_path(fp)\n    if not lang:\n        continue","negative_doc_ids":["retrieval/ast_chunker.py:116-132"],"negative_texts":["def collect_files(roots:List[str])->List[str]:\n    import fnmatch\n    out=[]\n    skip_dirs = {\".git\",\"node_modules\",\".venv\",\"venv\",\"dist\",\"build\",\"__pycache__\",\".next\",\".turbo\",\".parcel-cache\",\".pytest_cache\",\"vendor\",\"third_party\",\".bundle\",\"Pods\"}\n    exclude_patterns = []\n    for root in roots:\n        parent_dir = os.path.dirname(root) if os.path.isfile(root) else root\n        exclude_file = os.path.join(parent_dir, 'data', 'exclude_globs.txt')\n        if os.path.exists(exclude_file):\n            try:\n                with open(exclude_file, 'r') as f:\n                    patterns = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n                    exclude_patterns.extend(patterns)\n            except Exception:\n                pass"]}
{"query":"Exclude Markdown from indexing (policy vs parser map)","positive_doc_id":"indexer/index_repo.py:84-90","positive_text":"SOURCE_EXTS = {\n    \".py\", \".rb\", \".ts\", \".tsx\", \".js\", \".jsx\", \".go\", \".rs\", \".java\",\n    \".cs\", \".c\", \".h\", \".cpp\", \".hpp\", \".m\", \".mm\", \".kt\", \".kts\", \".swift\",\n    \".sql\", \".yml\", \".yaml\", \".toml\", \".ini\", \".json\", \".txt\", \".sh\", \".bash\"\n    # Note: .md excluded per user requirement - filtering.py blocks it\n}","negative_doc_ids":["retrieval/ast_chunker.py:11-20"],"negative_texts":["LANG_MAP = {\n    \".py\": \"python\", \".js\": \"javascript\", \".jsx\": \"javascript\",\n    \".ts\": \"typescript\", \".tsx\": \"typescript\",\n    \".go\": \"go\", \".java\": \"java\", \".rs\": \"rust\",\n    \".c\": \"c\", \".h\": \"c\", \".cpp\": \"cpp\", \".cc\": \"cpp\", \".hpp\": \"cpp\",\n    \".sh\": \"bash\", \".bash\": \"bash\",\n    \".txt\": \"text\",\n    \".yml\": \"yaml\", \".yaml\": \"yaml\",\n    \".md\": \"markdown\",\n}"]}
{"query":"Async streaming index run sets cwd and env","positive_doc_id":"server/services/indexing.py:60-71","positive_text":"env = os.environ.copy()\nenv['REPO'] = repo\nenv['REPO_ROOT'] = str(repo_root())\nenv['SKIP_DENSE'] = '0' if dense else '1'\ncmd = [sys.executable, '-m', 'indexer.index_repo']\nproc = await asyncio.create_subprocess_exec(\n    *cmd,\n    stdout=asyncio.subprocess.PIPE,\n    stderr=asyncio.subprocess.STDOUT,\n    cwd=str(repo_root()),\n    env=env\n)","negative_doc_ids":["server/routers/indexing.py:13-15"],"negative_texts":["@router.post(\"/api/index/start\")\ndef index_start(payload: Dict[str, Any] = None) -> Dict[str, Any]:\n    return svc.start(payload)"]}
{"query":"Index stats compute per-profile sizes (backend source of truth)","positive_doc_id":"server/index_stats.py:124-140","positive_text":"repo_stats: Dict[str, Any] = {\n    \"name\": repo_name,\n    \"profile\": profile_name,\n    \"paths\": {\n        \"chunks\": str(chunks_file) if chunks_file.exists() else None,\n        \"bm25\": str(bm25_dir) if bm25_dir.exists() else None,\n        \"cards\": str(cards_file) if cards_file.exists() else None,\n    },\n    \"sizes\": {},\n    \"chunk_count\": 0,\n    \"has_cards\": cards_file.exists() if cards_file else False,\n}\n# Aggregate sizes and counts\nif chunks_file.exists():\n    size = chunks_file.stat().st_size\n    repo_stats[\"sizes\"][\"chunks\"] = size","negative_doc_ids":["gui/app.js:1823-1826"],"negative_texts":["const totalSize = (repo.sizes.chunks || 0) + (repo.sizes.bm25 || 0) + (repo.sizes.cards || 0);\nhtml.push(`\n    <div style=\"background:var(--code-bg);border:1px solid ${repo.has_cards ? 'var(--accent)' : 'var(--line)'};border-radius:6px;padding:12px;margin-bottom:8px;\">\n`)]}
{"query":"Patch os.walk to prune dirs (indexer) vs raw collection (retrieval)","positive_doc_id":"indexer/index_repo.py:30-37","positive_text":"# Patch os.walk to prune noisy dirs and skip junk file types\n_os_walk = os.walk\ndef _filtered_os_walk(top, *args, **kwargs):\n    for root, dirs, files in _os_walk(top, *args, **kwargs):\n        _prune_dirs_in_place(dirs)\n        files[:] = [f for f in files if _should_index_file(f)]\n        yield root, dirs, files\nos.walk = _filtered_os_walk  # type: ignore","negative_doc_ids":["retrieval/ast_chunker.py:131-138"],"negative_texts":["for root in roots:\n    for dp, dns, fns in os.walk(root):\n        dns[:] = [d for d in dns if d not in skip_dirs and not d.startswith('.venv') and not d.startswith('venv')]\n        for fn in fns:\n            p = os.path.join(dp, fn)\n            skip = False\n            for pattern in exclude_patterns:\n                if fnmatch.fnmatch(p, pattern) or fnmatch.fnmatch(os.path.relpath(p, root), pattern):\n                    skip = True\n                    break"]}
{"query":"Persist last_index.json metadata (writer) vs timestamp reader","positive_doc_id":"indexer/index_repo.py:350-360","positive_text":"meta = {\n    'repo': REPO,\n    'timestamp': datetime.utcnow().isoformat() + 'Z',\n    'chunks_path': os.path.join(OUTDIR, 'chunks.jsonl'),\n    'bm25_index_dir': os.path.join(OUTDIR, 'bm25_index'),\n    'chunk_count': len(chunks),\n    'collection_name': COLLECTION,\n}\nwith open(os.path.join(OUTDIR, 'last_index.json'), 'w', encoding='utf-8') as mf:\n    json.dump(meta, mf, indent=2)","negative_doc_ids":["server/index_stats.py:17-25"],"negative_texts":["def _last_index_timestamp_for_repo(base: Path, repo_name: str) -> str | None:\n    \"\"\"Return the best-effort last index timestamp for a single repo under a base dir.\n\n    Preference order:\n    1) base/<repo>/last_index.json[\"timestamp\"]\n    2) mtime of base/<repo>/chunks.jsonl\n    3) mtime of base/<repo>/bm25_index directory\n    \"\"\"\n    repo_dir = base / repo_name"]}
{"query":"Compute chunk hash vs deriving UUID for points","positive_doc_id":"indexer/index_repo.py:288-293","positive_text":"h = hashlib.md5(c['code'].encode()).hexdigest()\nif h in seen:\n    continue\nseen.add(h)\nc['hash'] = h\nchunks.append(c)","negative_doc_ids":["indexer/index_repo.py:421-427"],"negative_texts":["cid = str(c['id'])\npid = str(uuid.uuid5(uuid.NAMESPACE_DNS, cid))\nslim_payload = {\n    'id': c.get('id'),\n    'file_path': c.get('file_path'),\n    'start_line': c.get('start_line'),\n    'end_line': c.get('end_line'),"]}
{"query":"Convert absolute to repo-relative file paths","positive_doc_id":"indexer/index_repo.py:263-271","positive_text":"# Convert absolute path to relative path (for portability)\n# Try to make it relative to one of the repo base paths\nrelative_fp = fp\nfor base in BASES:\n    try:\n        relative_fp = str(Path(fp).relative_to(Path(base)))\n        break\n    except ValueError:\n        # fp is not relative to this base, try next one","negative_doc_ids":["retrieval/ast_chunker.py:131-142"],"negative_texts":["for root in roots:\n    for dp, dns, fns in os.walk(root):\n        dns[:] = [d for d in dns if d not in skip_dirs and not d.startswith('.venv') and not d.startswith('venv')]\n        for fn in fns:\n            p = os.path.join(dp, fn)\n            # ... appends absolute paths to out\n        if not skip and lang_from_path(p):\n            out.append(p)"]}
