---
sidebar_position: 1
---

# Model Configuration

Complete guide to choosing embedding and inference models for AGRO based on your requirements, budget, and hardware.

:::warning AI Model Pricing Changes Daily
These recommendations are current as of **October 8, 2025**, but will become outdated quickly. Always verify pricing and performance at official sources before making decisions.
:::

## Quick Decision Matrix

| Use Case | Embedding | Inference | Monthly Cost* | Hardware |
|----------|-----------|-----------|---------------|----------|
| **Production (Cloud)** | OpenAI text-embedding-3-large | GPT-4o mini | $5 | None |
| **Budget Cloud** | Google Gemini (free tier) | Gemini 2.5 Flash | $1 | None |
| **Premium Cloud** | OpenAI text-embedding-3-large | Claude Sonnet 4.5 | $10 | None |
| **Self-Hosted (Mac M-series)** | nomic-embed-text | Qwen3-Coder 30B | $0 | 32GB+ RAM |
| **Self-Hosted (NVIDIA)** | NV-Embed-v2 | DeepSeek-Coder V2 | $0 | 24GB+ VRAM |
| **Privacy First** | BGE-M3 (local) | Qwen3-Coder (local) | $0 | 16GB+ RAM |

*Estimated for high usage (1M tokens/month). AGRO caches by default and only re-embeds changed code.

## Cloud Models (API-Based)

:::tip Important Note
You do NOT need the same level of inference model that you would use without a good RAG. AGRO does a lot of the intelligence work upfront through semantic search and reranking.
:::

### Current API Pricing

:::warning Prices Change Frequently
Verify current pricing at:
- **OpenAI**: https://platform.openai.com/pricing
- **Google**: https://ai.google.dev/pricing
- **Anthropic**: https://docs.anthropic.com/pricing
:::

#### OpenAI (Updated October 2025)

:::info API Change
OpenAI has deprecated the Chat Completions API in favor of the Responses API. If an LLM recommends Chat Completions, note this is outdated.
:::

| Model | Input (per 1M tokens) | Output (per 1M tokens) | Use Case |
|-------|----------------------|------------------------|----------|
| **GPT-4o mini** | $0.15 | $0.60 | Best value for code generation |
| **GPT-4o** | $2.50 | $10.00 | Production, complex reasoning |
| **o3-mini** | $0.40 | $1.60 | Fast reasoning (new) |
| **text-embedding-3-large** | $0.13 | N/A | High-quality embeddings (3072d) |
| **text-embedding-3-small** | $0.02 | N/A | Budget embeddings (1536d) |

#### Google Gemini (Updated October 2025)

| Model | Input (per 1M tokens) | Output (per 1M tokens) | Use Case |
|-------|----------------------|------------------------|----------|
| **Gemini 2.5 Flash** | $0.075 | $0.30 | Best value, 1M context |
| **Gemini 2.5 Pro** | $1.25 | $5.00 | Advanced reasoning |
| **Gemini Embeddings** | $0.15 | N/A | Free tier available (768d) |

**Free Tier**: Gemini offers generous free limits for embeddings. Check current quotas at https://ai.google.dev/pricing

#### Anthropic Claude (Updated October 2025)

| Model | Input (per 1M tokens) | Output (per 1M tokens) | Use Case |
|-------|----------------------|------------------------|----------|
| **Claude Haiku 3.5** | $0.80 | $4.00 | Fast, economical |
| **Claude Sonnet 4.5** | $3.00 | $15.00 | Balanced performance |
| **Claude Opus 4.1** | $15.00 | $75.00 | Highest quality |

### Cloud Model Recommendations

#### Best Overall Value: Gemini 2.5 Flash

- **Why**: $0.075/$0.30 pricing with 1M token context
- **When**: Production RAG with moderate budgets
- **Caveat**: Slightly lower code quality than GPT-4o mini

#### Best Code Quality: GPT-4o mini

- **Why**: 87.2% HumanEval score, $0.15/$0.60 pricing
- **When**: Code generation is critical
- **Caveat**: More expensive than Gemini

#### Best for Large Context: Gemini 2.5 Flash

- **Why**: 1M token context (8x more than GPT-4o mini)
- **When**: Processing large codebases or documents
- **Caveat**: Requires careful prompt engineering

## Self-Hosted Models

### Embedding Models

#### nomic-embed-text (Recommended for Mac)

- **Dimensions**: 768
- **Size**: 274MB
- **Performance**: 71% MTEB (surpasses OpenAI ada-002)
- **Hardware**: 8GB+ RAM (Mac M-series optimized)
- **Speed**: Very fast on Apple Silicon
- **Setup**: `ollama pull nomic-embed-text`

#### BGE-M3 (Multilingual, High Quality)

- **Dimensions**: 1024
- **Size**: ~1.3GB
- **Performance**: 71% MTEB
- **Hardware**: 16GB+ RAM
- **Languages**: 100+
- **Setup**: Via sentence-transformers

#### NV-Embed-v2 (NVIDIA GPUs)

- **Dimensions**: 1024
- **Size**: ~7GB
- **Performance**: 72.31% MTEB (former #1)
- **Hardware**: NVIDIA GPU with 16GB+ VRAM
- **Speed**: Optimized for CUDA
- **Setup**: Via transformers with `trust_remote_code`

### Inference Models

#### Qwen3-Coder (Recommended)

**Variants**:
- **7B** (~4GB): 8GB+ RAM
- **14B** (~8GB): 16GB+ RAM
- **30B** (~18GB): 32GB+ RAM (Mac M4 Max recommended)

**Performance**: Excellent on code (leads open-source)
**Context**: 256K tokens
**Speed**: 100+ tokens/sec on M4 Max

**Setup (Ollama)**:
```bash
ollama pull qwen3-coder:30b
```

**Setup (MLX - Apple Silicon)**:
```bash
pip install mlx mlx-lm
python -c "from mlx_lm import load; load('mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit')"
```

#### MLX vs Ollama (Apple Silicon)

**MLX**:
- Direct Metal GPU integration
- Optimized for Apple Silicon unified memory
- Uses GPU, NOT Neural Engine (ANE is for smaller CoreML models)
- Better memory efficiency
- Setup: `pip install mlx mlx-lm`

**Ollama**:
- Also uses Metal GPU on Apple Silicon
- Similar thermal profile to MLX
- Easier multi-platform support
- More straightforward model management
- Setup: `brew install ollama`

**Verdict**: Both are good options. MLX may have a slight memory efficiency edge, Ollama is more portable.

#### DeepSeek-Coder V3

**Variants**:
- **16B** (~9GB): 16GB+ RAM or 12GB+ VRAM
- **236B** (multi-GPU): 80GB+ VRAM

**Performance**: 85.6% HumanEval (highest open-source)
**Context**: 16K tokens

**Setup**:
```bash
ollama pull deepseek-coder-v2:16b
```

## Hardware-Specific Recommendations

### Apple Silicon Macs

#### M1/M2 (8-16GB RAM)

- **Embedding**: nomic-embed-text
- **Inference**: Qwen3-Coder 7B
- **Why**: Optimized for unified memory, fits in RAM

#### M3/M4 (16-32GB RAM)

- **Embedding**: nomic-embed-text or BGE-M3
- **Inference**: Qwen3-Coder 14B
- **Why**: More headroom for larger models

#### M4 Pro/Max (32GB+ RAM)

- **Embedding**: BGE-M3
- **Inference**: Qwen3-Coder 30B
- **Why**: Can handle state-of-the-art local models

### NVIDIA GPUs

#### RTX 3080/4080 (8-16GB VRAM)

- **Embedding**: BGE-large
- **Inference**: Qwen3-Coder 14B or DeepSeek-Coder 16B
- **Why**: Good balance for mid-range GPUs

#### RTX 3090/4090 (24GB VRAM)

- **Embedding**: NV-Embed-v2
- **Inference**: Qwen3-Coder 30B or DeepSeek-Coder 16B
- **Why**: Full utilization of high-end consumer GPUs

#### A100/H100 (40GB+ VRAM)

- **Embedding**: NV-Embed-v2
- **Inference**: DeepSeek-Coder V2 236B
- **Why**: Datacenter-grade, best local performance

### CPU-Only (Budget/Privacy)

#### 16-32GB RAM

- **Embedding**: BGE-small-en-v1.5
- **Inference**: Qwen3-Coder 7B (quantized)
- **Why**: CPU inference is slow, use smallest viable models
- **Note**: Expect 5-10x slower than GPU

## Benchmark References

:::warning Benchmarks Change Daily
These links show current leaderboards. Rankings shift as new models release.
:::

### Embedding Benchmarks

- **MTEB Leaderboard** (Primary): https://huggingface.co/spaces/mteb/leaderboard
- **NVIDIA Blog**: https://developer.nvidia.com/blog/nvidia-text-embedding-model-tops-mteb-leaderboard/
- **Nomic Analysis**: https://www.nomic.ai/blog/posts/evaluating-embedding-models

### Code Generation Benchmarks

- **HumanEval Stats**: https://llm-stats.com/benchmarks/humaneval
- **LiveBench**: https://livebench.ai/
- **Vellum LLM Leaderboard**: https://www.vellum.ai/llm-leaderboard
- **Aider Leaderboards**: https://aider.chat/docs/leaderboards/

## Migration Guides

### Switch to Fully Local (Mac Example)

**1. Install Ollama**

```bash
brew install ollama
ollama serve  # Keep running
```

**2. Pull Models**

```bash
# Embedding
ollama pull nomic-embed-text

# Inference (choose based on RAM)
ollama pull qwen3-coder:7b   # 8-16GB
ollama pull qwen3-coder:14b  # 16-32GB
ollama pull qwen3-coder:30b  # 32GB+
```

**3. Update .env**

```bash
# Generation
OLLAMA_URL=http://127.0.0.1:11434/api
GEN_MODEL=qwen3-coder:30b

# Embeddings (optional, uses Ollama)
EMBEDDING_TYPE=local  # Falls back to Ollama/local models
```

**4. Re-index**

```bash
REPO=agro python index_repo.py
```

### Switch to Budget Cloud (Gemini)

**1. Get API Key**

- Visit: https://makersuite.google.com/app/apikey
- Create key

**2. Update .env**

```bash
GOOGLE_API_KEY=your_key_here
EMBEDDING_TYPE=gemini  # If implementing Gemini embeddings
GEN_MODEL=gemini-2.5-flash
```

**3. Update Code** (if not already supported)

See implementation examples in the code. Gemini support may need custom integration.

## Final Recommendations by Scenario

### Startup/Prototype (Minimize Cost)

- **Embedding**: Google Gemini (free tier)
- **Inference**: Gemini 2.5 Flash
- **Why**: $3-10/month for moderate usage
- **When to Switch**: When free tier limits hit or need better quality

### Production (Balance Cost/Quality)

- **Embedding**: OpenAI text-embedding-3-large
- **Inference**: GPT-4o mini or Gemini 2.5 Flash
- **Why**: Proven reliability, good performance
- **Cost**: $50-200/month depending on scale

### Enterprise (Best Quality, On-Prem)

- **Embedding**: NV-Embed-v2 or BGE-M3
- **Inference**: Qwen3-Coder 30B or DeepSeek-Coder V2
- **Why**: Complete data control, zero API costs
- **Cost**: Hardware only ($1.4K-$3K Mac or GPU server)

### Privacy-Critical (Airgapped)

- **Embedding**: BGE-M3 (multilingual)
- **Inference**: Qwen3-Coder 14B
- **Why**: Fully offline, no external APIs
- **Cost**: Hardware only, works on M-series Mac or mid-range PC

## Additional Resources

- **OpenAI Pricing**: https://platform.openai.com/pricing
- **Google AI Pricing**: https://ai.google.dev/pricing
- **Anthropic Pricing**: https://docs.anthropic.com/pricing
- **MTEB Leaderboard**: https://huggingface.co/spaces/mteb/leaderboard
- **HumanEval Benchmarks**: https://llm-stats.com/benchmarks/humaneval
- **Ollama Models**: https://ollama.com/library
- **Continue.dev Embedding Guide**: https://docs.continue.dev/customize/model-roles/embeddings

---

**Last Updated**: October 8, 2025
**Next Review**: Check pricing/benchmarks before implementing - they change daily!
